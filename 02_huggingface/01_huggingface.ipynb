{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01_Hugging Face\n",
    "---\n",
    "## 01_01 Hugging Face란?\n",
    "* 코드로 구현되어 있는 모델을 모아놓은 곳으로 AI의 github라 불린다.\n",
    "* 허깅페이스는 최신 AI 기술을 더 간편하게 활용 할 수 있또록 제공되는 플랫폼이며, 핵심기술은 Transformers 라이브러리이다.\n",
    "\n",
    "### 01_01_01 Hugging Face 장점\n",
    "* 모델을 처음부터 개발하고 훈련할 필요가 없다.\n",
    "* 사용 편의성 (간단하고 사용자 친화적인 인터페이스가 제공되어 쉽게 이용가능)\n",
    "* 지속적으로 발전 중 (AI의 최신기술을 공유받고 쉽게 접근 가능)\n",
    "\n",
    "\n",
    "## 01_01 Transformers 라이브러리\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_01 자연어처리\n",
    "* 컴퓨터가 인간의 언어를 이해, 생성, 조작 할 수 있도록 해주는 인공지능의 한 분야\n",
    "\n",
    "**자연어 처리의 주요 단계**\n",
    "\n",
    "* 토큰화: 텍스트를 의미 있는 단위(토큰)로 나누는 과정  \n",
    "    예시: \"나는 학교에 간다\" -> [\"나는\", \"학교에\", \"간다\"]<br>\n",
    "\n",
    "* 단어 집합 생성: 텍스트에서 고유한 단어들을 모아 사전을 만드는 과정  \n",
    "    예시: [\"나는\", \"학교에\", \"간다\"] -> {\"나는\", \"학교에\", \"간다\"}<br>\n",
    "\n",
    "* 정수 인코딩: 단어를 고유한 정수로 변환하여 컴퓨터가 처리할 수 있게 하는 과정  \n",
    "    예시: {\"나는\": 1, \"학교에\": 2, \"간다\": 3} -> [1, 2, 3]<br>\n",
    "\n",
    "* 패딩: 서로 다른 길이의 시퀀스를 동일한 길이로 맞추는 과정  \n",
    "    예시: [1, 2, 3] -> [1, 2, 3, 0, 0] (길이 5로 패딩)<br>\n",
    "\n",
    "* 벡터화: 텍스트 데이터를 숫자 벡터로 변환하여 기계학습 모델에 입력할 수 있게 하는 과정  \n",
    "    예시: \"나는\" -> [0.1, 0.2, 0.3, ..., 0.9] (100차원 벡터)  \n",
    "\n",
    "<br><br>\n",
    "\n",
    "**형태소 토큰화**  \n",
    "<br>\n",
    "원시문 예시 :  \n",
    "\"길을 가다가 고양이를 봤어. 고양이에게 츄르를 주려했는데 고양이가 도망가버렸어\"  \n",
    "\n",
    "토큰화 예시 :  \n",
    "[\"길을\", \"가다가\", \"고양이를\", \"봤어\", \".\", \"고양이에게\", \"츄르를\", \"주려했는데\", \"고양이가\", \"도망가버렸어\"]  \n",
    "\n",
    "똑같은 고양이가 총 3번 등장한다, 이때 \"를\", \"에게\", \"가\" 가 붙어있어 기계는 각기 다른 단어로 인식 할 수 있다.  \n",
    "따라서 한국에서는 보편적으로 '형태소 분석기'로 토큰화를 진행한다.  \n",
    "\n",
    "형태소 토큰화 예시 : \n",
    "[\"길\", \"을\", \"가다가\", \"고양이\", \"를\", \"보\", \"았\", \"어\", \".\", \"고양이\", \"에게\", \"츄르\", \"를\", \"주\", \"려\", \"하\", \"았\", \"는데\", \"고양이\", \"가\", \"도망가\", \"버리\", \"었\", \"어\"]  \n",
    "\n",
    "이렇게 형태소 단위로 분리하면 \"고양이\"라는 단어가 일관되게 인식되며, 조사나 어미는 별도로 분리된다.  \n",
    "이를 통해 기계가 더 정확하게 단어의 의미와 문법적 기능을 파악할 수 있다. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 자연어 처리 발전 과정\n",
    "\n",
    "**카운트 기반 단어 표현**\n",
    "* 단어 표현 중 하나로, 단어의 빈도수를 기반으로 단어의 의미를 표현하는 방법\n",
    "* DTM(Document Term Matrix) : 문서 내 단어의 빈도수를 기반으로 문서를 표현하는 방법\n",
    "* TF-IDF : 단어의 중요도를 고려하여 문서를 표현하는 방법 <br>\n",
    "\n",
    "Bag of Words(단어주머니)\n",
    "* 문서 내 단어의 출현 여부를 기반으로 문서를 표현하는 방법\n",
    "* 단어의 출현 빈도로만 나타내기 때문에 단어의 순서는 고려하지 않는다.\n",
    "<br> => 문맥 파악이 불가능 (강아지가 놀고 있다 와 놀고 있는 강아지를 구별 못함)\n",
    "\n",
    "**단어임베딩(통계기반의 언어모델)**\n",
    "* 대표적인 알고리즘 : Word2vec, GloVe\n",
    "* 단어를 벡터로 표현하는 방법\n",
    "* 단어의 의미를 벡터로 표현하여 문맥을 고려할 수 있도록 해준다.\n",
    "<br> => 하지만 다의어 같은 (문맥에따라 다양한의미를 기지는걸 구별하지 못함)\n",
    "\n",
    "**딥러닝 기반 언어 모델**\n",
    "\n",
    "**시퀀스 모델**\n",
    "* 시퀀스란?\n",
    "<br>시퀀스는 순서가 있는 데이터의 집합을 의미한다.\n",
    "<br>자연어 처리에서 시퀀스는 주로 단어나 문자의 연속을 나타낸다.\n",
    "<br>예를 들어, 문장은 단어들의 시퀀스이고, 단어는 문자들의 시퀀스입니다.\n",
    "<br>시퀀스 데이터는 순서가 중요하며, 이전 요소가 다음 요소에 영향을 미칠 수 있다.<br>\n",
    "\n",
    "* RNN(Recurrent Neural Network)<br> \n",
    "순차적 데이터를 처리하는 신경망으로, 이전 단계의 정보를 현재 단계로 전달하는 구조를 가짐\n",
    "* LSTM(Long Short-Term Memory)<br>\n",
    "RNN의 장기 의존성 문제를 해결하기 위해 개발된 모델로, 장기 기억을 유지할 수 있는 구조를 가짐\n",
    "* GRU(Gated Recurrent Unit)<br>\n",
    "LSTM을 간소화한 모델로, 더 적은 매개변수를 사용하면서도 비슷한 성능을 보임\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01_01_02 Transformers 라이브러리란?\n",
    "* 허깅페이스에서 제공하는 최신 모델을 쉽게 사용할 수 있도록 도와주는 라이브러리이다.\n",
    "* 트랜스포머는 자연어 처리 분야에서 많이 사용되는 모델로, 다양한 언어 모델을 포함하고 있다.\n",
    "\n",
    "트랜스포머는 자연어 처리 분야의 주요 딥러닝 모델로, \"Attention is All You Need\" 논문에서 소개되었다.  \n",
    "Self-Attention 메커니즘을 사용하여 문맥 이해와 단어 간 관계 파악에 효과적이다.  \n",
    "기계 번역, 텍스트 생성, 질문 응답 등 다양한 작업에서 우수한 성능을 보인다.\n",
    "\n",
    "쉽게 생각하면 컴퓨터가 말하는 방법을 배우는 것과 비슷한 것으로 문장 속의 단어들이 어떤 관계를 가지고 있는지 파악하고 문장의 뜻을 이해하는데 기술이다\n",
    "\n",
    "* Transformers를 통해 사전학습된 최신 모델을 쉽게 다운로드하고 학습할 수 있는 API와 도구를 제공한다.\n",
    "\n",
    "**지원하는 기능**\n",
    "1) 자연어 처리 : 텍스트 분류, 엔티티 인식, 질문답변, 언어모델링, 요약, 번역, 객관식 텍스트 생성\n",
    "2) 컴퓨터 비전 : 이미지 분류, 물체감지 및 분할\n",
    "3) 오디오 : 음성인식 및 오디오 분류\n",
    "4) 멀티모달 : 테이블 질문 답변, 광학문자 인식, 스캔문서에서 정보추출, 비디오분류, 시각적 질문답변\n",
    "\n",
    "멀티모달이란 : 텍스트 이미지, 음성, 영상 등 다양한 양식으로 훈련해 다양한 결과물을 낼 수 있는 모델\n",
    "-> 즉, 여러가지 유형의 데이터를 동시에 다루는 기술\n",
    "\n",
    "https://huggingface.co/docs/transformers/index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "파이토치\n",
    "트랜스포머스"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8211, 0.7149, 0.8858],\n",
      "        [0.9558, 0.0493, 0.4232],\n",
      "        [0.5838, 0.6757, 0.6158],\n",
      "        [0.1942, 0.6849, 0.0397],\n",
      "        [0.6471, 0.2461, 0.7083]])\n",
      "4.45.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.rand(5,3)\n",
    "print(x)\n",
    "\n",
    "import transformers\n",
    "print(transformers.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01-01-02. 사용 방법\n",
    "\n",
    "**pipeline() 함수**\n",
    "- 다양한 task를 위한 인터페이스를 제공하여, 복잡한 모델을 쉽게 사용할 수 있게 해준다.\n",
    "- 특정 모델과 동작에 필요한 전 처리 및 후 처리 단계를 연결하여 텍스트를 직접 입력하고 이해하기 쉬운 답변을 얻을 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to facebook/wav2vec2-base-960h and revision 22aad52 (https://huggingface.co/facebook/wav2vec2-base-960h).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8d542b53ab403b97c1a20584754e68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\huggingface_hub\\file_download.py:139: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\80403\\.cache\\huggingface\\hub\\models--facebook--wav2vec2-base-960h. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9f667e7e4f6468b8485322edaba257e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/378M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-base-960h and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3f2364a14414bcb99cc0ff24354229c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/163 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27dae9b8a42641c092cec9af88ceb27a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/291 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5809178a2dd4d11bf465db6f8f7c84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/85.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbefb7f9ca4b4c3f8aedbd4ed9adc074",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/159 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# transformers 패키지에서 pipeline을 import\n",
    "from transformers import pipeline\n",
    "\n",
    "# 음성을 인식해서 텍스트로 출력할 수 있는 모델\n",
    "# STT (automatic-speech-recognition) : 음성 인식\n",
    "# pipeline에 작업을 인자로 주면 자동으로 적합한 모델과 전 처리 클래스를 불러온다.\n",
    "transcriber = pipeline('automatic-speech-recognition')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "ffmpeg was not found but is required to load audio files from filename",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\audio_utils.py:34\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 34\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mffmpeg_command\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msubprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPIPE\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m ffmpeg_process:\n\u001b[0;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\subprocess.py:1026\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, user, group, extra_groups, encoding, errors, text, umask, pipesize, process_group)\u001b[0m\n\u001b[0;32m   1023\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mTextIOWrapper(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstderr,\n\u001b[0;32m   1024\u001b[0m                     encoding\u001b[38;5;241m=\u001b[39mencoding, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreexec_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1027\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mpass_fds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1028\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshell\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1029\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mp2cread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp2cwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1030\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mc2pread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2pwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1031\u001b[0m \u001b[43m                        \u001b[49m\u001b[43merrread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrwrite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1032\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mrestore_signals\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1033\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mgid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mumask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1034\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mstart_new_session\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprocess_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1035\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[0;32m   1036\u001b[0m     \u001b[38;5;66;03m# Cleanup if the child failed starting.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\subprocess.py:1538\u001b[0m, in \u001b[0;36mPopen._execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_gid, unused_gids, unused_uid, unused_umask, unused_start_new_session, unused_process_group)\u001b[0m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1538\u001b[0m     hp, ht, pid, tid \u001b[38;5;241m=\u001b[39m \u001b[43m_winapi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCreateProcess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexecutable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1539\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;66;43;03m# no special security\u001b[39;49;00m\n\u001b[0;32m   1540\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1541\u001b[0m \u001b[43m                             \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mclose_fds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1542\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcreationflags\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1543\u001b[0m \u001b[43m                             \u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1544\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mcwd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1545\u001b[0m \u001b[43m                             \u001b[49m\u001b[43mstartupinfo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1546\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m   1547\u001b[0m     \u001b[38;5;66;03m# Child is launched. Close the parent's copy of those pipe\u001b[39;00m\n\u001b[0;32m   1548\u001b[0m     \u001b[38;5;66;03m# handles that only the child should have open.  You need\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1551\u001b[0m     \u001b[38;5;66;03m# pipe will not close when the child process exits and the\u001b[39;00m\n\u001b[0;32m   1552\u001b[0m     \u001b[38;5;66;03m# ReadFile will hang.\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 지정된 파일을 찾을 수 없습니다",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 마틴루터킹 목사 연설 중 일부\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# I  have a dream that one day this nation will rise up and live out the true meaning of its creed\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[43mtranscriber\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mhttps://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:284\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.__call__\u001b[1;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    223\u001b[0m     inputs: Union[np\u001b[38;5;241m.\u001b[39mndarray, \u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m    224\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    225\u001b[0m ):\n\u001b[0;32m    226\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    Transcribe the audio sequence(s) given as inputs to text. See the [`AutomaticSpeechRecognitionPipeline`]\u001b[39;00m\n\u001b[0;32m    228\u001b[0m \u001b[38;5;124;03m    documentation for more information.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    282\u001b[0m \u001b[38;5;124;03m                `\"\".join(chunk[\"text\"] for chunk in output[\"chunks\"])`.\u001b[39;00m\n\u001b[0;32m    283\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\base.py:1260\u001b[0m, in \u001b[0;36mPipeline.__call__\u001b[1;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterate(inputs, preprocess_params, forward_params, postprocess_params)\n\u001b[0;32m   1259\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframework \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m, ChunkPipeline):\n\u001b[1;32m-> 1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1261\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1262\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1263\u001b[0m \u001b[43m                \u001b[49m\u001b[43m[\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreprocess_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforward_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpostprocess_params\u001b[49m\n\u001b[0;32m   1264\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1265\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrun_single(inputs, preprocess_params, forward_params, postprocess_params)\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:124\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_item()\n\u001b[0;32m    123\u001b[0m \u001b[38;5;66;03m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m--> 124\u001b[0m item \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    125\u001b[0m processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(item, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    126\u001b[0m \u001b[38;5;66;03m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:269\u001b[0m, in \u001b[0;36mPipelinePackIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    266\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m accumulator\n\u001b[0;32m    268\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_last:\n\u001b[1;32m--> 269\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloader_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    271\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(processed, torch\u001b[38;5;241m.\u001b[39mTensor):\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    707\u001b[0m ):\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:33\u001b[0m, in \u001b[0;36m_IterableDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index:\n\u001b[0;32m     32\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 33\u001b[0m         data\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset_iter\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m     35\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\pt_utils.py:186\u001b[0m, in \u001b[0;36mPipelineChunkIterator.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n\u001b[0;32m    184\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    185\u001b[0m     \u001b[38;5;66;03m# Try to return next item\u001b[39;00m\n\u001b[1;32m--> 186\u001b[0m     processed \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msubiterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    188\u001b[0m     \u001b[38;5;66;03m# When a preprocess iterator ends, we can start lookig at the next item\u001b[39;00m\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# ChunkIterator will keep feeding until ALL elements of iterator\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    192\u001b[0m     \u001b[38;5;66;03m# Another way to look at it, is we're basically flattening lists of lists\u001b[39;00m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;66;03m# into a single list, but with generators\u001b[39;00m\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msubiterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfer(\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterator), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams)\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\automatic_speech_recognition.py:361\u001b[0m, in \u001b[0;36mAutomaticSpeechRecognitionPipeline.preprocess\u001b[1;34m(self, inputs, chunk_length_s, stride_length_s)\u001b[0m\n\u001b[0;32m    358\u001b[0m             inputs \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(inputs, \u001b[38;5;28mbytes\u001b[39m):\n\u001b[1;32m--> 361\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m \u001b[43mffmpeg_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msampling_rate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    363\u001b[0m stride \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    364\u001b[0m extra \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32mc:\\Users\\80403\\miniforge3\\envs\\lecture_env\\Lib\\site-packages\\transformers\\pipelines\\audio_utils.py:37\u001b[0m, in \u001b[0;36mffmpeg_read\u001b[1;34m(bpayload, sampling_rate)\u001b[0m\n\u001b[0;32m     35\u001b[0m         output_stream \u001b[38;5;241m=\u001b[39m ffmpeg_process\u001b[38;5;241m.\u001b[39mcommunicate(bpayload)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m---> 37\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mffmpeg was not found but is required to load audio files from filename\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merror\u001b[39;00m\n\u001b[0;32m     38\u001b[0m out_bytes \u001b[38;5;241m=\u001b[39m output_stream[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     39\u001b[0m audio \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mfrombuffer(out_bytes, np\u001b[38;5;241m.\u001b[39mfloat32)\n",
      "\u001b[1;31mValueError\u001b[0m: ffmpeg was not found but is required to load audio files from filename"
     ]
    }
   ],
   "source": [
    "# 마틴루터킹 목사 연설 중 일부\n",
    "# I  have a dream that one day this nation will rise up and live out the true meaning of its creed\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lecture_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
